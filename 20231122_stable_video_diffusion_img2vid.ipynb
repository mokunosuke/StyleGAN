{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e52f6d8a2ad4507a6ec0698bb74144d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d5f7fff4f654f07ad3f991e9483841d",
              "IPY_MODEL_4ce0db54a47e4e45ac455d7085f8bea1",
              "IPY_MODEL_3a1ce7ac430e4cfc9da5e29085aad7fd"
            ],
            "layout": "IPY_MODEL_fb6f039b7b8441d488b9560796762bc1"
          }
        },
        "5d5f7fff4f654f07ad3f991e9483841d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d035c834b71545f68ed8c6024ec059c5",
            "placeholder": "​",
            "style": "IPY_MODEL_682254c0dd4043d79202a0a24ad2cc63",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "4ce0db54a47e4e45ac455d7085f8bea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf8e8a61f2e45e6a6dd468440c6d743",
            "max": 3944692325,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba2a128a30e84e91b4779a5f88e6de3b",
            "value": 3944692325
          }
        },
        "3a1ce7ac430e4cfc9da5e29085aad7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1b3ec464a1047f6b167b64adde81827",
            "placeholder": "​",
            "style": "IPY_MODEL_1617f7320b4442819df91d2626e966d7",
            "value": " 3.94G/3.94G [00:15&lt;00:00, 204MB/s]"
          }
        },
        "fb6f039b7b8441d488b9560796762bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d035c834b71545f68ed8c6024ec059c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682254c0dd4043d79202a0a24ad2cc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bf8e8a61f2e45e6a6dd468440c6d743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2a128a30e84e91b4779a5f88e6de3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1b3ec464a1047f6b167b64adde81827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1617f7320b4442819df91d2626e966d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mokunosuke/StyleGAN/blob/main/20231122_stable_video_diffusion_img2vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注：V100などのGPU環境で実行する必要があります。16GB以下で動くことを確認しています。\n",
        "\n",
        "Example\n",
        "https://www.youtube.com/watch?v=v9DyHMmmxg4\n",
        "\n",
        "Original\n",
        "https://twitter.com/mk1stats/status/1727207950434083017\n",
        "\n"
      ],
      "metadata": {
        "id": "DzyOcvIR6FhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkshing/notebooks/blob/main/stable_video_diffusion_img2vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "# Stable Video Diffusion (image-to-video) Demo\n",
        "This notebook is the demo for the new image-to-video model, Stable Video Diffusion, from [Stability AI](https://stability.ai/) **on Colab free plan**.\n",
        "\n",
        "This was made by [mkshing](https://twitter.com/mk1stats).\n",
        "\n",
        "Visit the following links for the details of Stable Video Diffusion.\n",
        "* Codebase: https://github.com/Stability-AI/generative-models\n",
        "* HF: https://huggingface.co/stabilityai/stable-video-diffusion-img2vid\n",
        " * LICENSE: [STABLE VIDEO DIFFUSION NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/LICENSE)\n",
        "* Paper: https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n",
        "\n",
        "![000000](https://user-images.githubusercontent.com/33302880/284800538-f856b437-aa1f-4675-ba40-03da3e953358.gif)\n",
        "\n"
      ],
      "metadata": {
        "id": "QIuzds5LLPyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "!nvidia-smi\n",
        "!git clone https://github.com/Stability-AI/generative-models.git\n",
        "# install required packages from pypi\n",
        "# !pip3 install -r generative-models/requirements/pt2.txt\n",
        "# manually install only necesarry packages for colab\n",
        "!wget https://gist.githubusercontent.com/mkshing/4ad40699756d996ba6b3f7934e6ca532/raw/3f0094272c7a2bd3eb5f1a0db91bed582c9e8f01/requirements.txt\n",
        "!pip3 install -r requirements.txt\n",
        "!pip3 install -e generative-models\n",
        "!pip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n",
        "!pip3 install gradio"
      ],
      "metadata": {
        "id": "aaimSFWfLPgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9688311d-0075-447d-d087-4edcbcfe232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 10 01:50:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Cloning into 'generative-models'...\n",
            "remote: Enumerating objects: 941, done.\u001b[K\n",
            "remote: Total 941 (delta 0), reused 0 (delta 0), pack-reused 941\u001b[K\n",
            "Receiving objects: 100% (941/941), 43.85 MiB | 30.74 MiB/s, done.\n",
            "Resolving deltas: 100% (490/490), done.\n",
            "--2024-05-10 01:50:35--  https://gist.githubusercontent.com/mkshing/4ad40699756d996ba6b3f7934e6ca532/raw/3f0094272c7a2bd3eb5f1a0db91bed582c9e8f01/requirements.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 746 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     746  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-10 01:50:35 (47.0 MB/s) - ‘requirements.txt’ saved [746/746]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting clip@ git+https://github.com/openai/CLIP.git (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-i2quqvkv/clip_4c6135142c6c4d79bf44652a1dd47ad4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-i2quqvkv/clip_4c6135142c6c4d79bf44652a1dd47ad4\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black==23.7.0 (from -r requirements.txt (line 2))\n",
            "  Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==5.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.1 (from -r requirements.txt (line 5))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale>=0.4.13 (from -r requirements.txt (line 6))\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire>=0.5.0 (from -r requirements.txt (line 7))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2023.6.0)\n",
            "Collecting invisible-watermark>=0.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia==0.6.9 (from -r requirements.txt (line 10))\n",
            "  Downloading kornia-0.6.9-py2.py3-none-any.whl (569 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib>=3.7.2 (from -r requirements.txt (line 11))\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: natsort>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (8.4.0)\n",
            "Collecting ninja>=1.11.1 (from -r requirements.txt (line 13))\n",
            "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "Collecting omegaconf>=2.3.0 (from -r requirements.txt (line 15))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting open-clip-torch>=2.20.0 (from -r requirements.txt (line 16))\n",
            "  Downloading open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.6.0.66 (from -r requirements.txt (line 17))\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (2.0.3)\n",
            "Collecting pillow>=9.5.0 (from -r requirements.txt (line 19))\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pudb>=2022.1.3 (from -r requirements.txt (line 20))\n",
            "  Downloading pudb-2024.1.tar.gz (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning==2.0.1 (from -r requirements.txt (line 21))\n",
            "  Downloading pytorch_lightning-2.0.1-py3-none-any.whl (716 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.4/716.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.11.4)\n",
            "Collecting streamlit>=0.73.1 (from -r requirements.txt (line 24))\n",
            "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardx==2.6 (from -r requirements.txt (line 25))\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm>=0.9.2 (from -r requirements.txt (line 26))\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.12.1 (from -r requirements.txt (line 27))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (2.2.1+cu121)\n",
            "Collecting torchdata==0.6.1 (from -r requirements.txt (line 30))\n",
            "  Downloading https://download.pytorch.org/whl/torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=1.0.1 (from -r requirements.txt (line 31))\n",
            "  Downloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (4.66.4)\n",
            "Collecting transformers==4.19.1 (from -r requirements.txt (line 34))\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from -r requirements.txt (line 35))\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab hack for SVD\n",
        "# !pip uninstall -y numpy\n",
        "# !pip install -U numpy\n",
        "!mkdir -p /content/scripts/util/detection\n",
        "!ln -s /content/generative-models/scripts/util/detection/p_head_v1.npz /content/scripts/util/detection/p_head_v1.npz\n",
        "!ln -s /content/generative-models/scripts/util/detection/w_head_v1.npz /content/scripts/util/detection/w_head_v1.npz"
      ],
      "metadata": {
        "id": "zWlfaXvPbR1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このハックで リポジトリから来た npzを読ませている (NumPy: Save and load arrays as binary file)"
      ],
      "metadata": {
        "id": "eFuaCvM08liV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v8O2yR3BLHv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064f82b5-47b5-435c-c2f5-d18ffaa2e841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download from https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/resolve/main/svd_xt.safetensors to checkpoints/svd_xt.safetensors\n"
          ]
        }
      ],
      "source": [
        "# @title Download weights\n",
        "import os\n",
        "import subprocess\n",
        "version = \"svd\" #@param [\"svd\", \"svd_xt\"]\n",
        "TYPE2PATH = {\n",
        "    \"svd\": [\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/resolve/main/svd.safetensors\", \"checkpoints/svd.safetensors\"],\n",
        "    \"svd_xt\": [\"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/resolve/main/svd_xt.safetensors\", \"checkpoints/svd_xt.safetensors\"]\n",
        "}\n",
        "download_from, download_to = TYPE2PATH[version]\n",
        "# @markdown This will take several minutes. <br>\n",
        "# @markdown **Reference:**\n",
        "# @markdown * `svd`: [stabilityai/stable-video-diffusion-img2vid](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid) for 14 frames generation\n",
        "# @markdown * `svd_xt`: [stabilityai/stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) for 25 frames generation\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "if os.path.exists(download_to):\n",
        "  print(\"Already downloaded\")\n",
        "else:\n",
        "  print(f\"download from {download_from} to {download_to}\")\n",
        "  subprocess.call([\"wget\", download_from, \"-O\", download_to])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Model\n",
        "import sys\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import torch\n",
        "\n",
        "sys.path.append(\"generative-models\")\n",
        "from sgm.util import default, instantiate_from_config\n",
        "from scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\n",
        "\n",
        "def load_model(\n",
        "    config: str,\n",
        "    device: str,\n",
        "    num_frames: int,\n",
        "    num_steps: int,\n",
        "):\n",
        "    config = OmegaConf.load(config)\n",
        "    config.model.params.conditioner_config.params.emb_models[\n",
        "        0\n",
        "    ].params.open_clip_embedding_config.params.init_device = device\n",
        "    config.model.params.sampler_config.params.num_steps = num_steps\n",
        "    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n",
        "        num_frames\n",
        "    )\n",
        "    with torch.device(device):\n",
        "        model = instantiate_from_config(config.model).to(device).eval().requires_grad_(False)\n",
        "\n",
        "    filter = DeepFloydDataFiltering(verbose=False, device=device)\n",
        "    return model, filter\n",
        "\n",
        "\n",
        "if version == \"svd\":\n",
        "    num_frames = 14\n",
        "    num_steps = 25\n",
        "    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd/\")\n",
        "    model_config = \"generative-models/scripts/sampling/configs/svd.yaml\"\n",
        "elif version == \"svd_xt\":\n",
        "    num_frames = 25\n",
        "    num_steps = 30\n",
        "    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd_xt/\")\n",
        "    model_config = \"generative-models/scripts/sampling/configs/svd_xt.yaml\"\n",
        "else:\n",
        "    raise ValueError(f\"Version {version} does not exist.\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, filter = load_model(\n",
        "    model_config,\n",
        "    device,\n",
        "    num_frames,\n",
        "    num_steps,\n",
        ")\n",
        "# move models expect unet to cpu\n",
        "model.conditioner.cpu()\n",
        "model.first_stage_model.cpu()\n",
        "# change the dtype of unet\n",
        "model.model.to(dtype=torch.float16)\n",
        "torch.cuda.empty_cache()\n",
        "model = model.requires_grad_(False)"
      ],
      "metadata": {
        "id": "9AZDrh-SUDt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "2e52f6d8a2ad4507a6ec0698bb74144d",
            "5d5f7fff4f654f07ad3f991e9483841d",
            "4ce0db54a47e4e45ac455d7085f8bea1",
            "3a1ce7ac430e4cfc9da5e29085aad7fd",
            "fb6f039b7b8441d488b9560796762bc1",
            "d035c834b71545f68ed8c6024ec059c5",
            "682254c0dd4043d79202a0a24ad2cc63",
            "0bf8e8a61f2e45e6a6dd468440c6d743",
            "ba2a128a30e84e91b4779a5f88e6de3b",
            "c1b3ec464a1047f6b167b64adde81827",
            "1617f7320b4442819df91d2626e966d7"
          ]
        },
        "outputId": "c1759999-f3f9-4b0e-b70e-dba0a920eccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n",
            "VideoTransformerBlock is using checkpointing\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e52f6d8a2ad4507a6ec0698bb74144d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
            "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
            "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
            "Restored from checkpoints/svd_xt.safetensors with 0 missing and 0 unexpected keys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:29<00:00, 32.1MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampling function\n",
        "import math\n",
        "import os\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from einops import rearrange, repeat\n",
        "from fire import Fire\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "from sgm.inference.helpers import embed_watermark\n",
        "from sgm.util import default, instantiate_from_config\n",
        "\n",
        "\n",
        "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
        "    return list(set([x.input_key for x in conditioner.embedders]))\n",
        "\n",
        "\n",
        "def get_batch(keys, value_dict, N, T, device, dtype=None):\n",
        "    batch = {}\n",
        "    batch_uc = {}\n",
        "\n",
        "    for key in keys:\n",
        "        if key == \"fps_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"fps_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"motion_bucket_id\":\n",
        "            batch[key] = (\n",
        "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
        "                .to(device, dtype=dtype)\n",
        "                .repeat(int(math.prod(N)))\n",
        "            )\n",
        "        elif key == \"cond_aug\":\n",
        "            batch[key] = repeat(\n",
        "                torch.tensor([value_dict[\"cond_aug\"]]).to(device, dtype=dtype),\n",
        "                \"1 -> b\",\n",
        "                b=math.prod(N),\n",
        "            )\n",
        "        elif key == \"cond_frames\":\n",
        "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
        "        elif key == \"cond_frames_without_noise\":\n",
        "            batch[key] = repeat(\n",
        "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
        "            )\n",
        "        else:\n",
        "            batch[key] = value_dict[key]\n",
        "\n",
        "    if T is not None:\n",
        "        batch[\"num_video_frames\"] = T\n",
        "\n",
        "    for key in batch.keys():\n",
        "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
        "            batch_uc[key] = torch.clone(batch[key])\n",
        "    return batch, batch_uc\n",
        "\n",
        "\n",
        "\n",
        "def sample(\n",
        "    input_path: str = \"assets/test_image.png\",  # Can either be image file or folder with image files\n",
        "    resize_image: bool = False,\n",
        "    num_frames: Optional[int] = None,\n",
        "    num_steps: Optional[int] = None,\n",
        "    fps_id: int = 6,\n",
        "    motion_bucket_id: int = 127,\n",
        "    cond_aug: float = 0.02,\n",
        "    seed: int = 23,\n",
        "    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "    device: str = \"cuda\",\n",
        "    output_folder: Optional[str] = \"/content/outputs\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n",
        "    image file in folder `input_path`. If you run out of VRAM, try decreasing `decoding_t`.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    path = Path(input_path)\n",
        "    all_img_paths = []\n",
        "    if path.is_file():\n",
        "        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n",
        "            all_img_paths = [input_path]\n",
        "        else:\n",
        "            raise ValueError(\"Path is not valid image file.\")\n",
        "    elif path.is_dir():\n",
        "        all_img_paths = sorted(\n",
        "            [\n",
        "                f\n",
        "                for f in path.iterdir()\n",
        "                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
        "            ]\n",
        "        )\n",
        "        if len(all_img_paths) == 0:\n",
        "            raise ValueError(\"Folder does not contain any images.\")\n",
        "    else:\n",
        "        raise ValueError\n",
        "    all_out_paths = []\n",
        "    for input_img_path in all_img_paths:\n",
        "        with Image.open(input_img_path) as image:\n",
        "            if image.mode == \"RGBA\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            if resize_image and image.size != (1024, 576):\n",
        "                print(f\"Resizing {image.size} to (1024, 576)\")\n",
        "                image = TF.resize(TF.resize(image, 1024), (576, 1024))\n",
        "            w, h = image.size\n",
        "\n",
        "            if h % 64 != 0 or w % 64 != 0:\n",
        "                width, height = map(lambda x: x - x % 64, (w, h))\n",
        "                image = image.resize((width, height))\n",
        "                print(\n",
        "                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n",
        "                )\n",
        "\n",
        "            image = ToTensor()(image)\n",
        "            image = image * 2.0 - 1.0\n",
        "\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        H, W = image.shape[2:]\n",
        "        assert image.shape[1] == 3\n",
        "        F = 8\n",
        "        C = 4\n",
        "        shape = (num_frames, C, H // F, W // F)\n",
        "        if (H, W) != (576, 1024):\n",
        "            print(\n",
        "                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n",
        "            )\n",
        "        if motion_bucket_id > 255:\n",
        "            print(\n",
        "                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n",
        "            )\n",
        "\n",
        "        if fps_id < 5:\n",
        "            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n",
        "\n",
        "        if fps_id > 30:\n",
        "            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n",
        "\n",
        "        value_dict = {}\n",
        "        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
        "        value_dict[\"fps_id\"] = fps_id\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        value_dict[\"cond_frames_without_noise\"] = image\n",
        "        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
        "        value_dict[\"cond_aug\"] = cond_aug\n",
        "        # low vram mode\n",
        "        model.conditioner.cpu()\n",
        "        model.first_stage_model.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.sampler.verbose = True\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device):\n",
        "                model.conditioner.to(device)\n",
        "                batch, batch_uc = get_batch(\n",
        "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
        "                    value_dict,\n",
        "                    [1, num_frames],\n",
        "                    T=num_frames,\n",
        "                    device=device,\n",
        "                )\n",
        "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
        "                    batch,\n",
        "                    batch_uc=batch_uc,\n",
        "                    force_uc_zero_embeddings=[\n",
        "                        \"cond_frames\",\n",
        "                        \"cond_frames_without_noise\",\n",
        "                    ],\n",
        "                )\n",
        "                model.conditioner.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # from here, dtype is fp16\n",
        "                for k in [\"crossattn\", \"concat\"]:\n",
        "                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
        "                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
        "                for k in uc.keys():\n",
        "                    uc[k] = uc[k].to(dtype=torch.float16)\n",
        "                    c[k] = c[k].to(dtype=torch.float16)\n",
        "\n",
        "                randn = torch.randn(shape, device=device, dtype=torch.float16)\n",
        "\n",
        "                additional_model_inputs = {}\n",
        "                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n",
        "                    2, num_frames\n",
        "                ).to(device, )\n",
        "                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
        "\n",
        "                for k in additional_model_inputs:\n",
        "                    if isinstance(additional_model_inputs[k], torch.Tensor):\n",
        "                        additional_model_inputs[k] = additional_model_inputs[k].to(dtype=torch.float16)\n",
        "\n",
        "                def denoiser(input, sigma, c):\n",
        "                    return model.denoiser(\n",
        "                        model.model, input, sigma, c, **additional_model_inputs\n",
        "                    )\n",
        "\n",
        "                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
        "                samples_z.to(dtype=model.first_stage_model.dtype)\n",
        "                ##\n",
        "\n",
        "                model.en_and_decode_n_samples_a_time = decoding_t\n",
        "                model.first_stage_model.to(device)\n",
        "                samples_x = model.decode_first_stage(samples_z)\n",
        "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                model.first_stage_model.cpu()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                os.makedirs(output_folder, exist_ok=True)\n",
        "                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n",
        "                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n",
        "                writer = cv2.VideoWriter(\n",
        "                    video_path,\n",
        "                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
        "                    fps_id + 1,\n",
        "                    (samples.shape[-1], samples.shape[-2]),\n",
        "                )\n",
        "\n",
        "                samples = embed_watermark(samples)\n",
        "                samples = filter(samples)\n",
        "                vid = (\n",
        "                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n",
        "                    .cpu()\n",
        "                    .numpy()\n",
        "                    .astype(np.uint8)\n",
        "                )\n",
        "                for frame in vid:\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    writer.write(frame)\n",
        "                writer.release()\n",
        "                all_out_paths.append(video_path)\n",
        "    return all_out_paths\n",
        "\n"
      ],
      "metadata": {
        "id": "x1-dnq0RT95O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Do the Run!\n",
        "# @markdown Generation takes about 10 mins for `svd_xt` on T4 (Colab free plan). Please be patient...\n",
        "import gradio as gr\n",
        "import random\n",
        "\n",
        "\n",
        "def infer(input_path: str, resize_image: bool, n_frames: int, n_steps: int, seed: str, decoding_t: int) -> str:\n",
        "  if seed == \"random\":\n",
        "    seed = random.randint(0, 2**32)\n",
        "  seed = int(seed)\n",
        "  output_paths = sample(\n",
        "    input_path=input_path,\n",
        "    resize_image=resize_image,\n",
        "    num_frames=n_frames,\n",
        "    num_steps=n_steps,\n",
        "    fps_id=6,\n",
        "    motion_bucket_id=127,\n",
        "    cond_aug=0.02,\n",
        "    seed=23,\n",
        "    decoding_t=decoding_t,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
        "    device=device,\n",
        "  )\n",
        "  return output_paths[0]\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    image = gr.Image(label=\"input image\", type=\"filepath\")\n",
        "    resize_image = gr.Checkbox(label=\"resize to optimal size\", value=True)\n",
        "    btn = gr.Button(\"Run\")\n",
        "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
        "      n_frames = gr.Number(precision=0, label=\"number of frames\", value=num_frames)\n",
        "      n_steps = gr.Number(precision=0, label=\"number of steps\", value=num_steps)\n",
        "      seed = gr.Text(value=\"random\", label=\"seed (integer or 'random')\",)\n",
        "      decoding_t = gr.Number(precision=0, label=\"number of frames decoded at a time\", value=2)\n",
        "  with gr.Column():\n",
        "    video_out = gr.Video(label=\"generated video\")\n",
        "  examples = [\n",
        "      [\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"]\n",
        "  ]\n",
        "  inputs = [image, resize_image, n_frames, n_steps, seed, decoding_t]\n",
        "  outputs = [video_out]\n",
        "  btn.click(infer, inputs=inputs, outputs=outputs)\n",
        "  gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=infer)\n",
        "  demo.queue().launch(debug=True, share=True, show_error=True)"
      ],
      "metadata": {
        "id": "5MdVILPlMUDe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d947c480-fd6e-4838-c27b-d7b05fcb5cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://257293d32d8e7a84aa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://257293d32d8e7a84aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1520, in process_api\n",
            "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1303, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/components/image.py\", line 159, in preprocess\n",
            "    im = _Image.open(payload.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3283, in open\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1520, in process_api\n",
            "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1303, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/components/image.py\", line 159, in preprocess\n",
            "    im = _Image.open(payload.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3283, in open\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1520, in process_api\n",
            "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1303, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/components/image.py\", line 159, in preprocess\n",
            "    im = _Image.open(payload.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3283, in open\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1520, in process_api\n",
            "    inputs = self.preprocess_data(fn_index, inputs, state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1303, in preprocess_data\n",
            "    processed_input.append(block.preprocess(inputs_cached))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/components/image.py\", line 159, in preprocess\n",
            "    im = _Image.open(payload.path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3283, in open\n",
            "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: cannot identify image file '/tmp/gradio/a5f4f1a771a8d3d64a3f47e1fe00aef8a724eebb/20230913-VTubing.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (899, 899) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:04<00:04,  4.15s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (1856, 1280) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:05<00:04,  4.19s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (2912, 1632) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.02s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (1456, 816) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (2912, 1632) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing (2912, 1632) to (1024, 576)\n",
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  4.00s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.00s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.01s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.99s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.02s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.99s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.99s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 523, in reduce\n",
            "    return _apply_recipe(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 234, in _apply_recipe\n",
            "    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 187, in _reconstruct_from_shape_uncached\n",
            "    raise EinopsError(f\"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}\")\n",
            "einops.EinopsError: Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 208, in sample\n",
            "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
            "    x = self.sampler_step(\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
            "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 56, in denoise\n",
            "    denoised = self.guider(denoised, sigma)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/guiders.py\", line 81, in __call__\n",
            "    x_u = rearrange(x_u, \"(b t) ... -> b t ...\", t=self.num_frames)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\n",
            "    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\n",
            "    raise EinopsError(message + \"\\n {}\".format(e))\n",
            "einops.EinopsError:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 523, in reduce\n",
            "    return _apply_recipe(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 234, in _apply_recipe\n",
            "    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 187, in _reconstruct_from_shape_uncached\n",
            "    raise EinopsError(f\"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}\")\n",
            "einops.EinopsError: Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 208, in sample\n",
            "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
            "    x = self.sampler_step(\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
            "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 56, in denoise\n",
            "    denoised = self.guider(denoised, sigma)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/guiders.py\", line 81, in __call__\n",
            "    x_u = rearrange(x_u, \"(b t) ... -> b t ...\", t=self.num_frames)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\n",
            "    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\n",
            "    raise EinopsError(message + \"\\n {}\".format(e))\n",
            "einops.EinopsError:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 523, in reduce\n",
            "    return _apply_recipe(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 234, in _apply_recipe\n",
            "    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 187, in _reconstruct_from_shape_uncached\n",
            "    raise EinopsError(f\"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}\")\n",
            "einops.EinopsError: Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 208, in sample\n",
            "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
            "    x = self.sampler_step(\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
            "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 56, in denoise\n",
            "    denoised = self.guider(denoised, sigma)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/guiders.py\", line 81, in __call__\n",
            "    x_u = rearrange(x_u, \"(b t) ... -> b t ...\", t=self.num_frames)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\n",
            "    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\n",
            "    raise EinopsError(message + \"\\n {}\".format(e))\n",
            "einops.EinopsError:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 523, in reduce\n",
            "    return _apply_recipe(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 234, in _apply_recipe\n",
            "    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 187, in _reconstruct_from_shape_uncached\n",
            "    raise EinopsError(f\"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}\")\n",
            "einops.EinopsError: Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 208, in sample\n",
            "    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n",
            "    x = self.sampler_step(\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n",
            "    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 56, in denoise\n",
            "    denoised = self.guider(denoised, sigma)\n",
            "  File \"/content/generative-models/sgm/modules/diffusionmodules/guiders.py\", line 81, in __call__\n",
            "    x_u = rearrange(x_u, \"(b t) ... -> b t ...\", t=self.num_frames)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\n",
            "    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\n",
            "    raise EinopsError(message + \"\\n {}\".format(e))\n",
            "einops.EinopsError:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception:  Error while processing rearrange-reduction pattern \"(b t) ... -> b t ...\".\n",
            " Input tensor shape: torch.Size([4, 4, 72, 128]). Additional info: {'t': 25}.\n",
            " Shape mismatch, can't divide axis of length 4 in chunks of 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:03<00:04,  4.13s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.02s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:01<00:04,  4.05s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 674, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-10-0c436bd8481a>\", line 11, in infer\n",
            "    output_paths = sample(\n",
            "  File \"<ipython-input-9-cee3fd503a69>\", line 87, in sample\n",
            "    path = Path(input_path)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 960, in __new__\n",
            "    self = cls._from_parts(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 594, in _from_parts\n",
            "    drv, root, parts = self._parse_args(args)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 578, in _parse_args\n",
            "    a = os.fspath(a)\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.01s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [02:00<00:04,  4.03s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################  Sampling setting  ##############################\n",
            "Sampler: EulerEDMSampler\n",
            "Discretization: EDMDiscretization\n",
            "Guider: LinearPredictionGuider\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSampling with EulerEDMSampler for 31 steps:   0%|          | 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Sampling with EulerEDMSampler for 31 steps:  97%|█████████▋| 30/31 [01:59<00:03,  3.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:274: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}