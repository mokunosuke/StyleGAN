{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Search_for_Yui",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mokunosuke/StyleGAN/blob/main/Search_for_Yui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWtBaGbmLubM"
      },
      "source": [
        "# Search_for_Yui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bAVpVONtwUw"
      },
      "source": [
        "# セットアップ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9YGm8GcR1jQ"
      },
      "source": [
        "# --- githubよりコードを取得 ---\n",
        "%tensorflow_version 1.x\n",
        "!git clone https://github.com/cedro3/stylegan2.git\n",
        "%cd stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwU_4rN5SgZR"
      },
      "source": [
        "# --- クラスと関数の定義 ---\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import PIL.Image\n",
        "import sys\n",
        "import bz2\n",
        "from keras.utils import get_file\n",
        "import dlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import projector\n",
        "import pretrained_networks\n",
        "from training import dataset\n",
        "from training import misc\n",
        "\n",
        "# --------- ランドマーク検出 -------------\n",
        "class LandmarksDetector:\n",
        "    def __init__(self, predictor_model_path):\n",
        "        \"\"\"\n",
        "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
        "        \"\"\"\n",
        "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
        "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
        "\n",
        "    def get_landmarks(self, image):\n",
        "        img = dlib.load_rgb_image(image)\n",
        "        dets = self.detector(img, 1)\n",
        "\n",
        "        for detection in dets:\n",
        "            face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
        "            yield face_landmarks\n",
        "\n",
        "# ---------- 顔画像の切り出し --------------            \n",
        "def image_align(src_file, dst_file, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True):\n",
        "        # Align function from FFHQ dataset pre-processing step\n",
        "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "        src_file\n",
        "        dst_file \n",
        "        face_landmarks\n",
        "        output_size=1024 \n",
        "        transform_size=4096 \n",
        "        enable_padding=True\n",
        "            \n",
        "        lm = np.array(face_landmarks)\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        y = np.flipud(x) * [-1, 1]\n",
        "        c = eye_avg + eye_to_mouth * 0.1\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        if not os.path.isfile(src_file):\n",
        "            print('\\nCannot find source image. Please run \"--wilds\" before \"--align\".')\n",
        "            return\n",
        "        img = PIL.Image.open(src_file)\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        img.save(dst_file, 'PNG')\n",
        "\n",
        "\n",
        "# ------------ ファイルの解凍 ---------------        \n",
        "def unpack_bz2(src_path):\n",
        "    data = bz2.BZ2File(src_path).read()\n",
        "    dst_path = src_path[:-4]\n",
        "    with open(dst_path, 'wb') as fp:\n",
        "        fp.write(data)\n",
        "    return dst_path\n",
        "\n",
        "\n",
        "# ------------  潜在変数(ベクトル)の探索　------------\n",
        "def project_real_images(num_images): \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
        "    dataset_name = 'dataset'  \n",
        "    data_dir = 'my'  \n",
        "    num_snapshots = 5\n",
        "\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    proj = projector.Projector()\n",
        "    proj.set_network(Gs)\n",
        "\n",
        "    print('Loading images from \"%s\"...' % dataset_name)\n",
        "    dataset_obj = dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, repeat=False, shuffle_mb=0)\n",
        "    assert dataset_obj.shape == Gs.output_shape[1:]\n",
        "\n",
        "    os.makedirs('my/real_images', exist_ok=True)  \n",
        "    for image_idx in range(num_images):\n",
        "        print('Projecting image %d/%d ...' % (image_idx, num_images))\n",
        "        images, _labels = dataset_obj.get_minibatch_np(1)\n",
        "        images = misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n",
        "        \n",
        "        targets=images\n",
        "        png_prefix='./my/real_images/image'+str(image_idx)\n",
        "        num_snapshots=num_snapshots\n",
        "                \n",
        "        snapshot_steps = set(proj.num_steps - np.linspace(0, proj.num_steps, num_snapshots, endpoint=False, dtype=int))\n",
        "        misc.save_image_grid(targets, png_prefix + 'target.png', drange=[-1,1])\n",
        "        proj.start(targets)\n",
        "        while proj.get_cur_step() < proj.num_steps:\n",
        "            print('\\r%d / %d ... ' % (proj.get_cur_step(), proj.num_steps), end='', flush=True)\n",
        "            proj.step()\n",
        "            if proj.get_cur_step() in snapshot_steps:\n",
        "                misc.save_image_grid(proj.get_images(), png_prefix + 'step%04d.png' % proj.get_cur_step(), drange=[-1,1])\n",
        "            \n",
        "            if proj.get_cur_step() == proj.num_steps:  \n",
        "                vec = proj.get_dlatents() \n",
        "                if image_idx == 0:\n",
        "                   vec_syn = vec\n",
        "                else:\n",
        "                   vec_syn = np.concatenate([vec_syn, vec])  \n",
        "                print(vec_syn.shape)  \n",
        "        print('\\r%-30s\\r' % '', end='', flush=True)\n",
        "\n",
        "    return vec_syn\n",
        "\n",
        "# ------------- 2つのベクトル間を補完するアニメーションの作成 -------\n",
        "def generate_gif(vec_syn, idx):\n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True \n",
        "    Gs_syn_kwargs.truncation_psi = 0.5\n",
        "\n",
        "    image_gif = []\n",
        "    image_gif_256 = []\n",
        "    os.makedirs('my/gif', exist_ok=True)\n",
        "    for j in range(len(idx)-1):\n",
        "        for i in range(40):\n",
        "            vec = vec_syn[idx[j]]+(vec_syn[idx[j+1]]-vec_syn[idx[j]])*i/39\n",
        "            vec = vec.reshape(1, 18, 512)\n",
        "            images =  Gs.components.synthesis.run(vec, **Gs_syn_kwargs) \n",
        "            image_one = PIL.Image.fromarray(images[0], 'RGB')\n",
        "            image_gif.append(image_one)\n",
        "            image_gif_256.append(image_one.resize((256,256))) \n",
        "\n",
        "    image_gif[0].save('./my/gif/anime.gif', save_all=True, append_images=image_gif[1:],\n",
        "                      duration=100, loop=0)   \n",
        "    image_gif_256[0].save('./my/gif/anime_256.gif', save_all=True, append_images=image_gif_256[1:],\n",
        "                      duration=100, loop=0) \n",
        "\n",
        "# ------------　 フォルダー内の画像を表示　--------------\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "def display_pic(folder):\n",
        "    fig = plt.figure(figsize=(30, 40))\n",
        "    files = glob.glob(folder)\n",
        "    files.sort()\n",
        "    images=[]\n",
        "    for i in range(len(files)):\n",
        "        img = Image.open(files[i])    \n",
        "        images = np.asarray(img)\n",
        "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(images)\n",
        "        ax.imshow(image_plt)\n",
        "        ax.set_xlabel(str(i), fontsize=20)               \n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "# ------------------ ベクトルの画像化 ---------------\n",
        "def display(vec_syn):\n",
        "  \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True \n",
        "    Gs_syn_kwargs.truncation_psi = 0.5\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 40))   \n",
        "    for i in range(len(vec_syn)):\n",
        "        vec = vec_syn[i].reshape(1,18,512)\n",
        "        image =  Gs.components.synthesis.run(vec, **Gs_syn_kwargs)        \n",
        "        PIL.Image.fromarray(image[0], 'RGB')   \n",
        "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(image[0])\n",
        "        ax.imshow(image_plt)\n",
        "        ax.set_xlabel(str(i), fontsize=20)               \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "# make dirctory\n",
        "os.makedirs('my/pic', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Q9VLIF3kRM"
      },
      "source": [
        "# 顔画像切り出しモデルの読み込み\n",
        "LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
        "landmarks_model_path = unpack_bz2(get_file('shape_predictor_68_face_landmarks.dat.bz2',\n",
        "                                            LANDMARKS_MODEL_URL, cache_subdir='temp'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kovbSW_8_kr"
      },
      "source": [
        "#顔画像の切り出し\n",
        "sample/picにある画像データから顔画像を切り出してmy/picに保存します。\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ZV6vhd0Cmn"
      },
      "source": [
        "# 顔画像の切り出し\n",
        "RAW_IMAGES_DIR = 'sample/pic'\n",
        "ALIGNED_IMAGES_DIR = 'my/pic'\n",
        "\n",
        "landmarks_detector = LandmarksDetector(landmarks_model_path)\n",
        "for img_name in os.listdir(RAW_IMAGES_DIR):\n",
        "    raw_img_path = os.path.join(RAW_IMAGES_DIR, img_name)\n",
        "    for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
        "        face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], i)\n",
        "        aligned_face_path = os.path.join(ALIGNED_IMAGES_DIR, face_img_name)\n",
        "        image_align(raw_img_path, aligned_face_path, face_landmarks)\n",
        "        \n",
        "display_pic('./my/pic/*.*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLy3vbC-8OOf"
      },
      "source": [
        "#マルチ解像度データセットの作成\n",
        "my/picフォルダーにある顔画像からマルチ解像度のデータセットを作成しmy/datasetフォルダーに保存します。\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfc9qX-RjgKW"
      },
      "source": [
        "!python dataset_tool.py create_from_images ./my/dataset ./my/pic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx3mTkCp-9o5"
      },
      "source": [
        "#新垣結衣を生成する潜在変数を探索する \n",
        "コードを実行するとmy/datasetフォルダーにあるマルチ解像度のデータセットから潜在変数wを探索します。\\\n",
        "途中経過の画像は **my/real_imagesフォルダー**に保存されます。\\\n",
        "探索した5つの潜在変数は**vec_syn**で返って来ます。\\\n",
        "※探索試行回数は300です。変更したい場合は、**projector.py** の **self.num_steps**の値をエディター等で変更して下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWDyDSJECsBR"
      },
      "source": [
        "vec_syn = project_real_images(5)  # ()内はマルチ解像度のデータセットを作成した時の画像枚数\n",
        "display_pic('./my/pic/*.*')  # ターゲット画像の表示\n",
        "display(vec_syn)  # 探索した潜在変数によって生成した画像"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Iz7wAI3kRR"
      },
      "source": [
        "# 探索した潜在変数の保存\n",
        "os.makedirs('my/vector', exist_ok=True)\n",
        "np.save('my/vector/vec_syn', vec_syn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE4As_Qn3kRS"
      },
      "source": [
        "# 探索した潜在変数の読み込み\n",
        "vec_syn = np.load('my/vector/vec_syn.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuW4tRi938c"
      },
      "source": [
        "#顔画像のトランジション\n",
        "コードを実行すると、**変数vec_syn**に格納されている潜在変数を元に、アニメーションを作成します。\\\n",
        "作成した gifアニメーションは**my/real_gifフォルダー**に保存されます。\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E-QtFwM756gv"
      },
      "source": [
        "from IPython.display import Image\n",
        "generate_gif(vec_syn,[1, 0, 3])  # 潜在変数を1番目→0番目→3番目へアニメーション\n",
        "Image('./my/gif/anime_256.gif', format='png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}