{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mokunosuke/StyleGAN/blob/main/stylegan3_edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d--K8BYm6wB4"
      },
      "outputs": [],
      "source": [
        "#@title セットアップ\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'stylegan3-editing'\n",
        "\n",
        "# githubからコード取得\n",
        "!git clone https://github.com/cedro3/stylegan3-editing.git $CODE_DIR\n",
        "\n",
        "# ninjaインストール\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "# pyrallis & CLIPインストール\n",
        "!pip install pyrallis\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "\n",
        "# ライブラリー・インポート\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import dataclasses\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from editing.interfacegan.face_editor import FaceEditor\n",
        "from editing.styleclip_global_directions import edit as styleclip_edit\n",
        "from models.stylegan3.model import GeneratorType\n",
        "from notebooks.notebook_utils import Downloader, ENCODER_PATHS, INTERFACEGAN_PATHS, STYLECLIP_PATHS\n",
        "from notebooks.notebook_utils import run_alignment, crop_image, compute_transforms\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
        "from function import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "\n",
        "# 学習済みパラメータのダウンロード\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=False,\n",
        "                        subdir=\"pretrained_models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co9CS5uO6wB8"
      },
      "outputs": [],
      "source": [
        "#@title 初期設定\n",
        "\n",
        "# エンコーダタイプ選択\n",
        "experiment_type = 'restyle_pSp_ffhq' #@param ['restyle_e4e_ffhq', 'restyle_pSp_ffhq']\n",
        "\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"restyle_pSp_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_pSp_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"restyle_e4e_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_e4e_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "\n",
        "\n",
        "# エンコーダ・ダウンロード\n",
        "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "    print(f'Downloading ReStyle encoder model: {experiment_type}...')\n",
        "    try:\n",
        "      downloader.download_file(file_id=ENCODER_PATHS[experiment_type]['id'],\n",
        "                              file_name=ENCODER_PATHS[experiment_type]['name'])\n",
        "    except Exception as e:\n",
        "      raise ValueError(f\"Unable to download model correctly! {e}\")\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print(f'Model for {experiment_type} already exists!')\n",
        "\n",
        "\n",
        "# エンコーダ・ロード\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_encoder(checkpoint_path=model_path)\n",
        "avg_image = get_average_image(net)\n",
        "\n",
        "\n",
        "# --- 編集パラメータのダウンロード ---\n",
        "download_with_pydrive = False \n",
        "\n",
        "# download files for interfacegan\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/interfacegan/boundaries/ffhq\")\n",
        "print(\"Downloading InterFaceGAN boundaries...\")\n",
        "for editing_file, params in INTERFACEGAN_PATHS.items():\n",
        "    print(f\"Downloading {editing_file} boundary...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])\n",
        "\n",
        "# download files for styleclip\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/styleclip_global_directions/sg3-r-ffhq-1024\")\n",
        "print(\"Downloading StyleCLIP auxiliary files...\")\n",
        "for editing_file, params in STYLECLIP_PATHS.items():\n",
        "    print(f\"Downloading {editing_file}...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])\n",
        "    \n",
        "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXlMllxtkTQE"
      },
      "outputs": [],
      "source": [
        "#@title align & crop の作成\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "reset_folder('edit/align')\n",
        "reset_folder('edit/crop')\n",
        "\n",
        "files = sorted(os.listdir('edit/pic'))\n",
        "for i, file in enumerate(tqdm(files)):\n",
        "    input_image = run_alignment('edit/pic/'+file)\n",
        "    cropped_image =crop_image('edit/pic/'+file)\n",
        "    name = os.path.splitext(file)[0]\n",
        "    input_image.save('edit/align/'+name+'.jpg')\n",
        "    cropped_image.save('edit/crop/'+name+'.jpg')\n",
        "\n",
        "print('=== pic ===')\n",
        "display_pic('edit/pic')\n",
        "print('=== align ===')\n",
        "display_pic('edit/align')\n",
        "print('=== crop ===')\n",
        "display_pic('edit/crop')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMUljdqNSCCI"
      },
      "outputs": [],
      "source": [
        "#@title invert の作成\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "reset_folder('edit/invert')\n",
        "reset_folder('edit/latents')\n",
        "\n",
        "files = sorted(os.listdir('edit/align'))\n",
        "for file in tqdm(files):\n",
        "  input_image = Image.open('edit/align/'+file)\n",
        "  aligned_path = 'edit/align/'+file\n",
        "  cropped_path = 'edit/crop/'+file\n",
        "\n",
        "  landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)\n",
        "\n",
        "  opts.n_iters_per_batch = 3\n",
        "  opts.resize_outputs = False  # generate outputs at full resolution\n",
        "\n",
        "  img_transforms = EXPERIMENT_ARGS['transform']\n",
        "  transformed_image = img_transforms(input_image)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      tic = time.time()\n",
        "      result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
        "                                                net=net,\n",
        "                                                opts=opts,\n",
        "                                                avg_image=avg_image,\n",
        "                                                landmarks_transform=torch.from_numpy(landmarks_transform).cuda().float())\n",
        "      toc = time.time()\n",
        "      #print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "\n",
        "  result_tensors = result_batch[0]\n",
        "  final_rec = tensor2im(result_tensors[-1])#.resize(resize_amount)\n",
        "  final_rec.save('edit/invert/'+file)\n",
        "\n",
        "  name = os.path.splitext(file)[0]\n",
        "  np.save('edit/latents/'+name, result_latents[0][-1])\n",
        "  \n",
        "print('=== crop ===')\n",
        "display_pic('edit/crop')\n",
        "print('=== invert ===')\n",
        "display_pic('edit/invert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ETDJ9r7HZXz"
      },
      "outputs": [],
      "source": [
        "#@title InterFaceGANによる編集\n",
        "\n",
        "invert = '004.jpg'#@param {type:\"string\"}\n",
        "name = os.path.splitext(invert)[0]+'.npy'\n",
        "result_latents_ = np.load('edit/latents/'+name)\n",
        "\n",
        "aligned_path = 'edit/align/'+invert\n",
        "cropped_path = 'edit/crop/'+invert\n",
        "landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)\n",
        "\n",
        "edit_direction = 'age' #@param ['age', 'smile', 'pose', 'Male']\n",
        "min_value = -5 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "max_value = 5 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "\n",
        "\n",
        "#@title Perform Edit! { display-mode: \"form\" }\n",
        "print(f\"Performing edit for {edit_direction}...\")\n",
        "#input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
        "input_latent = torch.from_numpy(result_latents_).unsqueeze(0).cuda()\n",
        "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
        "                                        direction=edit_direction,\n",
        "                                        factor_range=(min_value, max_value),\n",
        "                                        user_transforms=landmarks_transform,\n",
        "                                        apply_user_transformations=True)\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "#@title Show Result { display-mode: \"form\" }\n",
        "def prepare_edited_result(edit_images):\n",
        "  if type(edit_images[0]) == list:\n",
        "      edit_images = [image[0] for image in edit_images]\n",
        "  res = np.array(edit_images[0].resize((512, 512)))\n",
        "  for image in edit_images[1:]:\n",
        "      res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
        "  res = Image.fromarray(res).convert(\"RGB\")\n",
        "  return res\n",
        "\n",
        "res = prepare_edited_result(edit_images)\n",
        "res\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTmQaSdJFy4A"
      },
      "outputs": [],
      "source": [
        "#@title StyleCLIPによる編集\n",
        "\n",
        "styleclip_args = styleclip_edit.EditConfig()\n",
        "global_direction_calculator = styleclip_edit.load_direction_calculator(stylegan_model=net.decoder, opts=styleclip_args)\n",
        "\n",
        "neutral_text = \"a face\" #@param {type:\"raw\"}\n",
        "target_text = \"a smiling face\" #@param {type:\"raw\"}\n",
        "alpha = 4 #@param {type:\"slider\", min:-5, max:5, step:0.5}\n",
        "beta = 0.13 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "\n",
        "\n",
        "# 設定\n",
        "opts = styleclip_edit.EditConfig()\n",
        "opts.alpha_min = alpha\n",
        "opts.alpha_max = alpha\n",
        "opts.num_alphas = 1\n",
        "opts.beta_min = beta\n",
        "opts.beta_max = beta\n",
        "opts.num_betas = 1\n",
        "opts.neutral_text = neutral_text\n",
        "opts.target_text = target_text\n",
        "\n",
        "# 推論\n",
        "input_latent = result_latents_\n",
        "input_transforms = torch.from_numpy(landmarks_transform).cpu().numpy()\n",
        "print(f'Performing edit for: \"{opts.target_text}\"...')\n",
        "edit_res, edit_latent = styleclip_edit.edit_image(latent=input_latent,\n",
        "                                                  landmarks_transform=input_transforms,\n",
        "                                                  stylegan_model=net.decoder,\n",
        "                                                  global_direction_calculator=global_direction_calculator,\n",
        "                                                  opts=opts,\n",
        "                                                  image_name=None,\n",
        "                                                  save=False)\n",
        "print(\"Done!\")\n",
        "\n",
        "input_image = Image.open('edit/invert/'+invert) ###\n",
        "transformed_image = img_transforms(input_image) ###\n",
        "\n",
        "# 表示\n",
        "input_im = tensor2im(transformed_image).resize((512, 512))\n",
        "edited_im = tensor2im(edit_res[0]).resize((512, 512))\n",
        "edit_coupled = np.concatenate([np.array(input_im), np.array(edited_im)], axis=1)\n",
        "edit_coupled = Image.fromarray(edit_coupled)\n",
        "edit_coupled.resize((1024, 512))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-kb1pgH_g4l"
      },
      "outputs": [],
      "source": [
        "# ビデオ編集（要PROハイメモリ）\n",
        "\n",
        "# shape_predictor copy\n",
        "import shutil\n",
        "shutil.copy('shape_predictor_68_face_landmarks.dat', 'pretrained_models/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "! python inversion/video/inference_on_video.py \\\n",
        "--video_path edit/video/02.mp4 \\\n",
        "--checkpoint_path pretrained_models/restyle_pSp_ffhq.pt \\\n",
        "--output_path out_02\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ビデオ再生\n",
        "video_path = 'out_02/edited_video_age_start_coupled.mp4' \n",
        "display_mp4(video_path)"
      ],
      "metadata": {
        "id": "tB_xIx_dTD1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "stylegan3_edit",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}